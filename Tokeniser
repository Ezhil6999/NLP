#  Natural Language Toolkit
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

text="Tokenization is the process of dividing text into tokens."
tokens=word_tokenize(text)
print(tokens)
